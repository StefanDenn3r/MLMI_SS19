{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XRay_on_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cjIFrxl4xpb",
        "colab_type": "text"
      },
      "source": [
        "# X-Ray Landmark Detection on Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYcltK7e6r5A",
        "colab_type": "text"
      },
      "source": [
        "## Preperation\n",
        "\n",
        "### Imports and installation of the required libraries\n",
        "\n",
        "The libraries tensorboardx and bayesian-optimization are not within the virtual environment of Google Colab, hence they have to be installed manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI-mX3ic4zBC",
        "colab_type": "code",
        "outputId": "81ecc52c-b833-497a-d8dc-0a67bd5f2479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "import os, glob\n",
        "\n",
        "! pip install tensorboardx\n",
        "! pip install bayesian-optimization\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▌                           | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 71kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 81kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 92kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardx) (41.0.1)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-1.8\n",
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.21.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.13.2)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo5Rut3WcSHH",
        "colab_type": "text"
      },
      "source": [
        "### Google Colab or Zip upload\n",
        "Either upload your project to Google Drive and mount it or upload project manually as .zip file and extract it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBFA_7f5cUe0",
        "colab_type": "code",
        "outputId": "07065e80-573b-439a-adc7-725dd3c1cdb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "use_google_drive = True\n",
        "\n",
        "if use_google_drive:\n",
        "  drive.mount('gdrive')\n",
        "  % cd gdrive/My\\ Drive/MLMI_SS19\n",
        "else:\n",
        "  file = files.upload()\n",
        "  file_path = os.path.join(ROOT,list(file.keys())[0])\n",
        "  zip_file = ZipFile(file_path)\n",
        "  zip_file.extractall(ROOT)\n",
        "  zip_file.close()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at gdrive\n",
            "/content/gdrive/My Drive/MLMI_SS19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRn8HOi7rSvV",
        "colab_type": "text"
      },
      "source": [
        "### Tensorboard and tunneling\n",
        "Install ngrok for tunneling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc71w6qerQtF",
        "colab_type": "code",
        "outputId": "76cb62bd-3cbc-42d6-dd57-c71d52fb9ea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "if os.path.exists(\"ngrok-stable-linux-amd64.zip\"):\n",
        "  os.remove(\"ngrok-stable-linux-amd64.zip\")\n",
        "\n",
        "if os.path.exists(\"ngrok\"):\n",
        "  os.remove(\"ngrok\")\n",
        "  \n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-10 19:36:36--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.165.51.142, 52.207.111.186, 34.196.237.103, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.165.51.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17556757 (17M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  16.74M  26.8MB/s    in 0.6s    \n",
            "\n",
            "2019-07-10 19:36:37 (26.8 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [17556757/17556757]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ6JZY18fS4G",
        "colab_type": "text"
      },
      "source": [
        "Start tensorboard and forward port with ngrok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kele9MJBfAVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = 'saved/log/'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fGr1nvVqduU",
        "colab_type": "text"
      },
      "source": [
        "Extract ngrok url for accessing tensorboard\n",
        "\n",
        "**Attention**: Sometimes it throws an error like this:\n",
        "```\n",
        "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
        "```\n",
        "If this is the case the easiest way to solve this issue is to delete the ngrok*.zip and ngrok from the Google Drive folder and install them again.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOJxnfekqPg2",
        "colab_type": "code",
        "outputId": "685a18ce-88ce-439f-9134-17ea2d9f8071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://b9960f3b.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8QsqbYA53MI",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1xgAd-K4Q30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from config import CONFIG  \n",
        "from parse_config import ConfigParser\n",
        "from train import main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsplbjDNM_eF",
        "colab_type": "text"
      },
      "source": [
        "### Handle IOError\n",
        "\n",
        "Google Colab has problems dealing with large amount of elements within a folder. Running it until it successfully loads will ensure there won't be an error later on. See [here](https://research.google.com/colaboratory/faq.html#drive-timeout) for further details.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEc0CEqcQ2OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = 'SubsetOnePatient' # 'OnePatient'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ1FzHqKLZU8",
        "colab_type": "code",
        "outputId": "a08efaea-58f0-47e1-83ca-b4b73f1af136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "while True:\n",
        "  try:\n",
        "    os.listdir(f'data/XRay/{data_folder}/Training/ABD_LYMPH_005')\n",
        "    os.listdir(f'data/XRay/{data_folder}/Validation/ABD_LYMPH_005')\n",
        "  except IOError:\n",
        "    print('IOError - keep running')\n",
        "  else:\n",
        "    print('succesfully accessed files')\n",
        "    break;"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "succesfully accessed files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L41jcwBrqkev",
        "colab_type": "text"
      },
      "source": [
        "### Manual Training\n",
        "Modify parameters and train model **manually**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hSLJkNjMZI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONFIG['arch']['args']['x_channels'] = 128\n",
        "CONFIG['arch']['args']['stage_channels'] = 512\n",
        "CONFIG['arch']['args']['num_stages'] = 5\n",
        "CONFIG['arch']['args']['dilation'] = 1\n",
        "CONFIG['arch']['args']['depthwise_separable_convolution'] = True\n",
        "\n",
        "CONFIG['data_loader']['args']['data_dir'] = f'data/XRay/{data_folder}'\n",
        "CONFIG['data_loader']['args']['batch_size'] = 1\n",
        "CONFIG['data_loader']['args']['validation_split'] = 0.2\n",
        "CONFIG['data_loader']['args']['shuffle'] = False\n",
        "CONFIG['data_loader']['args']['custom_args']['fraction_of_dataset'] = 1\n",
        "CONFIG['data_loader']['args']['custom_args']['sigma'] = 80\n",
        "CONFIG['data_loader']['args']['custom_args']['sigma_reduction_factor'] = 0.9\n",
        "\n",
        "CONFIG['optimizer']['args']['lr'] = 1e-5\n",
        "\n",
        "CONFIG['trainer']['epochs'] = 1000\n",
        "CONFIG['trainer']['save_period'] = 1\n",
        "CONFIG['trainer']['early_stop'] = 50\n",
        "\n",
        "CONFIG['prediction_blur'] = 2\n",
        "\n",
        "main(ConfigParser(CONFIG))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc_pc9Uht4Yv",
        "colab_type": "text"
      },
      "source": [
        "### Resume training\n",
        "By default it takes your last training run and the last model of it. \n",
        "If you want to use a specific run or a specific model you can provide it like this:\n",
        "\n",
        "```\n",
        "run_dir = \"0629_194146\"\n",
        "model_pth = \"checkpoint-epoch11.pth\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69wusrvmt3en",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa52b1f8-c47b-4e25-8787-9ad1adc7e4b4"
      },
      "source": [
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "base_saved_dir = \"saved/models/XRay\"\n",
        "\n",
        "run_dir = None\n",
        "model_pth = None\n",
        "\n",
        "for temp_run_dir in os.listdir(base_saved_dir)[::-1]:\n",
        "  if run_dir is None:\n",
        "      run_path = os.path.join(base_saved_dir, temp_run_dir)\n",
        "  else:\n",
        "    run_path = os.path.join(base_saved_dir, run_dir)\n",
        "\n",
        "  if model_pth is None:\n",
        "    model_path_list = glob.glob(f'{run_path}/checkpoint-epoch*.pth')\n",
        "    if not model_path_list:\n",
        "      continue\n",
        "    model_path = model_path_list[-1]\n",
        "    break\n",
        "  else:\n",
        "    model_path = os.path.join(run_path, model_pth)\n",
        "    break\n",
        "\n",
        "config = SourceFileLoader(\"CONFIG\", os.path.join(run_path, 'config.py')).load_module().CONFIG\n",
        "epoch = int(model_path.split('checkpoint-epoch')[-1][:-4])\n",
        "config['data_loader']['args']['custom_args']['sigma'] *= config['data_loader']['args']['custom_args']['sigma_reduction_factor'] ** epoch\n",
        "main(ConfigParser(config, model_path))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvolutionalPoseMachines(\n",
            "  (stage_1): Stage1(\n",
            "    (X): X(\n",
            "      (convs): ModuleList(\n",
            "        (0): Conv2d(1, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
            "        (1): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (3): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (convs): ModuleList(\n",
            "      (0): DepthwiseSeparableConvolution(\n",
            "        (depthwise): Conv2d(32, 32, kernel_size=(9, 9), stride=(1, 1))\n",
            "        (pointwise): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(512, 23, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (stages): ModuleList(\n",
            "    (0): StageN(\n",
            "      (X): X(\n",
            "        (convs): ModuleList(\n",
            "          (0): Conv2d(1, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
            "          (1): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "          (2): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "          (3): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (convs): ModuleList(\n",
            "        (0): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(55, 55, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(55, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (4): Conv2d(128, 23, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (1): StageN(\n",
            "      (X): X(\n",
            "        (convs): ModuleList(\n",
            "          (0): Conv2d(1, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
            "          (1): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "          (2): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "          (3): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (convs): ModuleList(\n",
            "        (0): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(55, 55, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(55, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (4): Conv2d(128, 23, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): StageN(\n",
            "      (X): X(\n",
            "        (convs): ModuleList(\n",
            "          (0): Conv2d(1, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
            "          (1): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "          (2): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "          (3): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (convs): ModuleList(\n",
            "        (0): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(55, 55, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(55, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (4): Conv2d(128, 23, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (3): StageN(\n",
            "      (X): X(\n",
            "        (convs): ModuleList(\n",
            "          (0): Conv2d(1, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
            "          (1): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "          (2): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(4, 4))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "          (3): DepthwiseSeparableConvolution(\n",
            "            (depthwise): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "            (pointwise): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2))\n",
            "            (relu): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (convs): ModuleList(\n",
            "        (0): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(55, 55, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(55, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): DepthwiseSeparableConvolution(\n",
            "          (depthwise): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1))\n",
            "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(5, 5))\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (4): Conv2d(128, 23, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 33497267\n",
            "Loading checkpoint: saved/models/XRay/0710_151137/checkpoint-epoch11.pth ...\n",
            "Checkpoint loaded. Resume training from epoch 12\n",
            "Current sigma: 22.594362918480005\n",
            "Train Epoch: 12 [0/200 (0%)] Loss: 0.002099\n",
            "Train Epoch: 12 [1/200 (0%)] Loss: 0.002245\n",
            "Train Epoch: 12 [2/200 (1%)] Loss: 0.002076\n",
            "Train Epoch: 12 [3/200 (2%)] Loss: 0.002017\n",
            "Train Epoch: 12 [4/200 (2%)] Loss: 0.002109\n",
            "Train Epoch: 12 [5/200 (2%)] Loss: 0.002264\n",
            "Train Epoch: 12 [6/200 (3%)] Loss: 0.002164\n",
            "Train Epoch: 12 [7/200 (4%)] Loss: 0.002165\n",
            "Train Epoch: 12 [8/200 (4%)] Loss: 0.002292\n",
            "Train Epoch: 12 [9/200 (4%)] Loss: 0.002321\n",
            "Train Epoch: 12 [10/200 (5%)] Loss: 0.002099\n",
            "Train Epoch: 12 [11/200 (6%)] Loss: 0.002378\n",
            "Train Epoch: 12 [12/200 (6%)] Loss: 0.002365\n",
            "Train Epoch: 12 [13/200 (6%)] Loss: 0.002090\n",
            "Train Epoch: 12 [14/200 (7%)] Loss: 0.002390\n",
            "Train Epoch: 12 [15/200 (8%)] Loss: 0.002280\n",
            "Train Epoch: 12 [16/200 (8%)] Loss: 0.002351\n",
            "Train Epoch: 12 [17/200 (8%)] Loss: 0.002150\n",
            "Train Epoch: 12 [18/200 (9%)] Loss: 0.002332\n",
            "Train Epoch: 12 [19/200 (10%)] Loss: 0.002115\n",
            "Train Epoch: 12 [20/200 (10%)] Loss: 0.002396\n",
            "Train Epoch: 12 [21/200 (10%)] Loss: 0.002088\n",
            "Train Epoch: 12 [22/200 (11%)] Loss: 0.002116\n",
            "Train Epoch: 12 [23/200 (12%)] Loss: 0.002156\n",
            "Train Epoch: 12 [24/200 (12%)] Loss: 0.002383\n",
            "Train Epoch: 12 [25/200 (12%)] Loss: 0.002202\n",
            "Train Epoch: 12 [26/200 (13%)] Loss: 0.002365\n",
            "Train Epoch: 12 [27/200 (14%)] Loss: 0.002157\n",
            "Train Epoch: 12 [28/200 (14%)] Loss: 0.002361\n",
            "Train Epoch: 12 [29/200 (14%)] Loss: 0.002104\n",
            "Train Epoch: 12 [30/200 (15%)] Loss: 0.002223\n",
            "Train Epoch: 12 [31/200 (16%)] Loss: 0.002121\n",
            "Train Epoch: 12 [32/200 (16%)] Loss: 0.002197\n",
            "Train Epoch: 12 [33/200 (16%)] Loss: 0.002398\n",
            "Train Epoch: 12 [34/200 (17%)] Loss: 0.002359\n",
            "Train Epoch: 12 [35/200 (18%)] Loss: 0.002038\n",
            "Train Epoch: 12 [36/200 (18%)] Loss: 0.002265\n",
            "Train Epoch: 12 [37/200 (18%)] Loss: 0.002319\n",
            "Train Epoch: 12 [38/200 (19%)] Loss: 0.002117\n",
            "Train Epoch: 12 [39/200 (20%)] Loss: 0.002321\n",
            "Train Epoch: 12 [40/200 (20%)] Loss: 0.002259\n",
            "Train Epoch: 12 [41/200 (20%)] Loss: 0.002373\n",
            "Train Epoch: 12 [42/200 (21%)] Loss: 0.002174\n",
            "Train Epoch: 12 [43/200 (22%)] Loss: 0.002319\n",
            "Train Epoch: 12 [44/200 (22%)] Loss: 0.002191\n",
            "Train Epoch: 12 [45/200 (22%)] Loss: 0.002357\n",
            "Train Epoch: 12 [46/200 (23%)] Loss: 0.002084\n",
            "Train Epoch: 12 [47/200 (24%)] Loss: 0.002284\n",
            "Train Epoch: 12 [48/200 (24%)] Loss: 0.002302\n",
            "Train Epoch: 12 [49/200 (24%)] Loss: 0.002355\n",
            "Train Epoch: 12 [50/200 (25%)] Loss: 0.002021\n",
            "Train Epoch: 12 [51/200 (26%)] Loss: 0.002395\n",
            "Train Epoch: 12 [52/200 (26%)] Loss: 0.002156\n",
            "Train Epoch: 12 [53/200 (26%)] Loss: 0.002055\n",
            "Train Epoch: 12 [54/200 (27%)] Loss: 0.002391\n",
            "Train Epoch: 12 [55/200 (28%)] Loss: 0.002057\n",
            "Train Epoch: 12 [56/200 (28%)] Loss: 0.002112\n",
            "Train Epoch: 12 [57/200 (28%)] Loss: 0.002247\n",
            "Train Epoch: 12 [58/200 (29%)] Loss: 0.002139\n",
            "Train Epoch: 12 [59/200 (30%)] Loss: 0.002389\n",
            "Train Epoch: 12 [60/200 (30%)] Loss: 0.002462\n",
            "Train Epoch: 12 [61/200 (30%)] Loss: 0.002339\n",
            "Train Epoch: 12 [62/200 (31%)] Loss: 0.002323\n",
            "Train Epoch: 12 [63/200 (32%)] Loss: 0.002013\n",
            "Train Epoch: 12 [64/200 (32%)] Loss: 0.002097\n",
            "Train Epoch: 12 [65/200 (32%)] Loss: 0.002013\n",
            "Train Epoch: 12 [66/200 (33%)] Loss: 0.002016\n",
            "Train Epoch: 12 [67/200 (34%)] Loss: 0.002161\n",
            "Train Epoch: 12 [68/200 (34%)] Loss: 0.002388\n",
            "Train Epoch: 12 [69/200 (34%)] Loss: 0.002065\n",
            "Train Epoch: 12 [70/200 (35%)] Loss: 0.002082\n",
            "Train Epoch: 12 [71/200 (36%)] Loss: 0.002314\n",
            "Train Epoch: 12 [72/200 (36%)] Loss: 0.002378\n",
            "Train Epoch: 12 [73/200 (36%)] Loss: 0.002275\n",
            "Train Epoch: 12 [74/200 (37%)] Loss: 0.002252\n",
            "Train Epoch: 12 [75/200 (38%)] Loss: 0.002310\n",
            "Train Epoch: 12 [76/200 (38%)] Loss: 0.002056\n",
            "Train Epoch: 12 [77/200 (38%)] Loss: 0.002060\n",
            "Train Epoch: 12 [78/200 (39%)] Loss: 0.002125\n",
            "Train Epoch: 12 [79/200 (40%)] Loss: 0.002183\n",
            "Train Epoch: 12 [80/200 (40%)] Loss: 0.002319\n",
            "Train Epoch: 12 [81/200 (40%)] Loss: 0.002345\n",
            "Train Epoch: 12 [82/200 (41%)] Loss: 0.002326\n",
            "Train Epoch: 12 [83/200 (42%)] Loss: 0.002118\n",
            "Train Epoch: 12 [84/200 (42%)] Loss: 0.002302\n",
            "Train Epoch: 12 [85/200 (42%)] Loss: 0.002219\n",
            "Train Epoch: 12 [86/200 (43%)] Loss: 0.002259\n",
            "Train Epoch: 12 [87/200 (44%)] Loss: 0.002080\n",
            "Train Epoch: 12 [88/200 (44%)] Loss: 0.002212\n",
            "Train Epoch: 12 [89/200 (44%)] Loss: 0.002009\n",
            "Train Epoch: 12 [90/200 (45%)] Loss: 0.002159\n",
            "Train Epoch: 12 [91/200 (46%)] Loss: 0.002323\n",
            "Train Epoch: 12 [92/200 (46%)] Loss: 0.002173\n",
            "Train Epoch: 12 [93/200 (46%)] Loss: 0.002318\n",
            "Train Epoch: 12 [94/200 (47%)] Loss: 0.002299\n",
            "Train Epoch: 12 [95/200 (48%)] Loss: 0.001976\n",
            "Train Epoch: 12 [96/200 (48%)] Loss: 0.002131\n",
            "Train Epoch: 12 [97/200 (48%)] Loss: 0.002233\n",
            "Train Epoch: 12 [98/200 (49%)] Loss: 0.002349\n",
            "Train Epoch: 12 [99/200 (50%)] Loss: 0.002296\n",
            "Train Epoch: 12 [100/200 (50%)] Loss: 0.002327\n",
            "Train Epoch: 12 [101/200 (50%)] Loss: 0.001973\n",
            "Train Epoch: 12 [102/200 (51%)] Loss: 0.002294\n",
            "Train Epoch: 12 [103/200 (52%)] Loss: 0.002365\n",
            "Train Epoch: 12 [104/200 (52%)] Loss: 0.002414\n",
            "Train Epoch: 12 [105/200 (52%)] Loss: 0.002225\n",
            "Train Epoch: 12 [106/200 (53%)] Loss: 0.002165\n",
            "Train Epoch: 12 [107/200 (54%)] Loss: 0.002139\n",
            "Train Epoch: 12 [108/200 (54%)] Loss: 0.002275\n",
            "Train Epoch: 12 [109/200 (54%)] Loss: 0.002367\n",
            "Train Epoch: 12 [110/200 (55%)] Loss: 0.002171\n",
            "Train Epoch: 12 [111/200 (56%)] Loss: 0.002152\n",
            "Train Epoch: 12 [112/200 (56%)] Loss: 0.002303\n",
            "Train Epoch: 12 [113/200 (56%)] Loss: 0.002479\n",
            "Train Epoch: 12 [114/200 (57%)] Loss: 0.002194\n",
            "Train Epoch: 12 [115/200 (58%)] Loss: 0.002073\n",
            "Train Epoch: 12 [116/200 (58%)] Loss: 0.002487\n",
            "Train Epoch: 12 [117/200 (58%)] Loss: 0.002212\n",
            "Train Epoch: 12 [118/200 (59%)] Loss: 0.002001\n",
            "Train Epoch: 12 [119/200 (60%)] Loss: 0.002245\n",
            "Train Epoch: 12 [120/200 (60%)] Loss: 0.002172\n",
            "Train Epoch: 12 [121/200 (60%)] Loss: 0.002341\n",
            "Train Epoch: 12 [122/200 (61%)] Loss: 0.002296\n",
            "Train Epoch: 12 [123/200 (62%)] Loss: 0.002195\n",
            "Train Epoch: 12 [124/200 (62%)] Loss: 0.002189\n",
            "Train Epoch: 12 [125/200 (62%)] Loss: 0.002410\n",
            "Train Epoch: 12 [126/200 (63%)] Loss: 0.002153\n",
            "Train Epoch: 12 [127/200 (64%)] Loss: 0.002064\n",
            "Train Epoch: 12 [128/200 (64%)] Loss: 0.002067\n",
            "Train Epoch: 12 [129/200 (64%)] Loss: 0.002367\n",
            "Train Epoch: 12 [130/200 (65%)] Loss: 0.002169\n",
            "Train Epoch: 12 [131/200 (66%)] Loss: 0.002362\n",
            "Train Epoch: 12 [132/200 (66%)] Loss: 0.002012\n",
            "Train Epoch: 12 [133/200 (66%)] Loss: 0.002510\n",
            "Train Epoch: 12 [134/200 (67%)] Loss: 0.002052\n",
            "Train Epoch: 12 [135/200 (68%)] Loss: 0.002224\n",
            "Train Epoch: 12 [136/200 (68%)] Loss: 0.002163\n",
            "Train Epoch: 12 [137/200 (68%)] Loss: 0.002440\n",
            "Train Epoch: 12 [138/200 (69%)] Loss: 0.002289\n",
            "Train Epoch: 12 [139/200 (70%)] Loss: 0.002355\n",
            "Train Epoch: 12 [140/200 (70%)] Loss: 0.002335\n",
            "Train Epoch: 12 [141/200 (70%)] Loss: 0.002217\n",
            "Train Epoch: 12 [142/200 (71%)] Loss: 0.002198\n",
            "Train Epoch: 12 [143/200 (72%)] Loss: 0.002183\n",
            "Train Epoch: 12 [144/200 (72%)] Loss: 0.002269\n",
            "Train Epoch: 12 [145/200 (72%)] Loss: 0.002216\n",
            "Train Epoch: 12 [146/200 (73%)] Loss: 0.002373\n",
            "Train Epoch: 12 [147/200 (74%)] Loss: 0.002252\n",
            "Train Epoch: 12 [148/200 (74%)] Loss: 0.002424\n",
            "Train Epoch: 12 [149/200 (74%)] Loss: 0.002207\n",
            "Train Epoch: 12 [150/200 (75%)] Loss: 0.002152\n",
            "Train Epoch: 12 [151/200 (76%)] Loss: 0.002167\n",
            "Train Epoch: 12 [152/200 (76%)] Loss: 0.002039\n",
            "Train Epoch: 12 [153/200 (76%)] Loss: 0.002086\n",
            "Train Epoch: 12 [154/200 (77%)] Loss: 0.002152\n",
            "Train Epoch: 12 [155/200 (78%)] Loss: 0.002085\n",
            "Train Epoch: 12 [156/200 (78%)] Loss: 0.002399\n",
            "Train Epoch: 12 [157/200 (78%)] Loss: 0.002205\n",
            "Train Epoch: 12 [158/200 (79%)] Loss: 0.002114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7vyQPQ5Q8aa",
        "colab_type": "text"
      },
      "source": [
        "### Bayesian Optimization\n",
        "Do **automatic** Bayesian optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO9SmoL7pFaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bayes_opt_train import run_bayes_opt\n",
        "\n",
        "run_bayes_opt({\n",
        "    'num_channels': (6, 8),  # {64, 128, 256}\n",
        "    'num_stacks': (2, 7),\n",
        "    'num_blocks': (1, 7),\n",
        "    'kernel_size': (1, 4),  # {3, 5, 7, 9}\n",
        "    'sigma': (0.6, 5),\n",
        "    'prediction_blur': (0.01, 1),\n",
        "    'threshold': (0.00001, 0.2),\n",
        "    'epochs': (200, 200)\n",
        "\n",
        "}, init_points=10, n_iter=10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}